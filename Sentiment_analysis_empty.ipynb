{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "In diesem Teil möchten wir uns mit Sentiment Analysis beschäftigen. Vereinfacht gesagt beschäftigt sich Sentiment Analysis damit, natürlichsprachliche Aussagen dahingehend zu bewerten, ob die subjektive Aussage des Sprechers positiv oder negativ wertend gemeint ist.\n",
    "\n",
    "Zu diesem Zweck haben wir den Datensatz von Sentiment140, einem Projekt der Stanford University, ausgewählt. Er beinhaltet 16 Millionen Tweets, die aufgrund der enthaltenen Emoticons automatisch in positiv und negativ eingeteilt wurden.\n",
    "\n",
    "Der Datensatz liegt als csv-Datei vor. Zunächst möchten wir uns die darin enthaltene Daten etwas genauer ansehen.\n",
    "\n",
    "## 1. Aufgabe\n",
    "\n",
    "1. Laden Sie den Datensatz von http://help.sentiment140.com/for-students\n",
    "2. Lesen Sie den Datensatz in eine Liste ein, benutzen Sie dazu den csv-reader: https://docs.python.org/3/library/csv.html. Da wir uns nur für die Felder `polarity` und `text` interessieren, sollte die Liste mit den Daten folgendes Format haben : `[(polarity, text),...]`. Positive Tweets haben eine polarity von '4', negative von '0'. Wandeln Sie beim Einlesen diese Werte gleich um in 1 (positiv) und 0 (negativ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, \"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\"), (0, \"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\"), (0, '@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds'), (0, 'my whole body feels itchy and like its on fire '), (0, \"@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \"), (0, '@Kwesidei not the whole crew '), (0, 'Need a hug '), (0, \"@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?\"), (0, \"@Tatiana_K nope they didn't have it \"), (0, '@twittera que me muera ? ')]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "polarities = []\n",
    "texts = []\n",
    "\n",
    "with open('data/training.1600000.processed.noemoticon.csv', 'r', encoding='iso-8859-1') as f:\n",
    "    reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "    for row in reader:\n",
    "        polarities.append(1 if int(row[0]) else 0)\n",
    "        texts.append(row[-1])\n",
    "        \n",
    "data = list(zip(polarities, texts))\n",
    "print(data[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Um einen Einblick in die Daten zu bekommen und um später ein Modell zur Sentiment Analyse trainieren zu können, sollen die Daten zunächst etwas aufbereitet werden. Da die sinntragenden Elemente in den Tweets die Wörter sind, sollten Sie die Tweets in Wörter aufteilen. Um genau zu sein, ist der Term 'Wörter' hier aus linguistischer Sicht etwas falsch, man spricht eigentlich von Tokens. Daher nennt man das Aufteilen von Text auch Tokenizing und die Funktion, die sowas kann, Tokenizer.\n",
    "Der allereinfachste Tokenizer ist vermutlich die `split` Methode. Tokenisieren Sie damit die eingelesen Tweets. Am Ende sollten Sie eine Liste `tokenized = [(polarity, [token_1,token_2, ...])]` erhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, ['@switchfoot', 'http://twitpic.com/2y1zl', '-', 'Awww,', \"that's\", 'a', 'bummer.', 'You', 'shoulda', 'got', 'David', 'Carr', 'of', 'Third', 'Day', 'to', 'do', 'it.', ';D']), (0, ['is', 'upset', 'that', 'he', \"can't\", 'update', 'his', 'Facebook', 'by', 'texting', 'it...', 'and', 'might', 'cry', 'as', 'a', 'result', 'School', 'today', 'also.', 'Blah!']), (0, ['@Kenichan', 'I', 'dived', 'many', 'times', 'for', 'the', 'ball.', 'Managed', 'to', 'save', '50%', 'The', 'rest', 'go', 'out', 'of', 'bounds']), (0, ['my', 'whole', 'body', 'feels', 'itchy', 'and', 'like', 'its', 'on', 'fire']), (0, ['@nationwideclass', 'no,', \"it's\", 'not', 'behaving', 'at', 'all.', \"i'm\", 'mad.', 'why', 'am', 'i', 'here?', 'because', 'I', \"can't\", 'see', 'you', 'all', 'over', 'there.']), (0, ['@Kwesidei', 'not', 'the', 'whole', 'crew']), (0, ['Need', 'a', 'hug']), (0, ['@LOLTrish', 'hey', 'long', 'time', 'no', 'see!', 'Yes..', 'Rains', 'a', 'bit', ',only', 'a', 'bit', 'LOL', ',', \"I'm\", 'fine', 'thanks', ',', \"how's\", 'you', '?']), (0, ['@Tatiana_K', 'nope', 'they', \"didn't\", 'have', 'it']), (0, ['@twittera', 'que', 'me', 'muera', '?'])]\n"
     ]
    }
   ],
   "source": [
    "tokens = [text.split() for text in texts]\n",
    "tokenized = list(zip(polarities, tokens))\n",
    "print(tokenized[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Abgesehen von natürlichsprachlichen Wörtern sind in Tweets mindestens auch Hashtags, Mentions und Links enthalten. Überlegen Sie sich, ob es Sinn ergibt, alle diese Bestandteile in den Daten in dieser From zu behalten. Begründen Sie kurz Ihre Entscheidungen.\n",
    "Falls Sie sich entschlossen haben, nicht alle diese Bestandteile zu behalten, filtern Sie dementsprechend Ihre Daten. Die Struktur Ihrer Daten sollte am Ende gleich bleiben: `cleaned = [(polarity, [token_1,...])]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*to be removed: Links, Mentions* <br/>\n",
    "*reformatting: everything to lowercase, remove all punctuation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGEX_URLS = r'(http(s)?:\\/\\/.)?(www\\.)?[-a-zA-Z0-9@:%.\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%\\+.~#?&//=]*)'\n",
    "REGEX_PUNCTUATION = r'[^\\w\\s\\']'  # Means everything except word chars (\\w), space chars (\\s) or apostrophes\n",
    "REGEX_EMOTICONS = r'(?::|;|=)(?:-)?(?:\\)|D|P)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_token(token):\n",
    "    '''\n",
    "    Check if Token is a URL or @mention. If so, return none. Else return token.\n",
    "    '''\n",
    "    #remove links\n",
    "    if re.match(REGEX_URLS, token):\n",
    "        #print(\"### Found URL in: \", token)\n",
    "        return\n",
    "        \n",
    "    if \"@\" in token:\n",
    "        #print(\"### Found @ in: \", token)\n",
    "        return\n",
    "    \n",
    "    #print(\"### Found nothing in: \", token)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_but_keep_emoticons(token):\n",
    "    '''\n",
    "    Removes all additional punctuation from a token but keeps it as a whole if it is an emoticon.\n",
    "    '''\n",
    "    if re.match(REGEX_EMOTICONS, token):\n",
    "        #print(\"### Found emoticon in: \", token)\n",
    "        return token\n",
    "    else:\n",
    "        return re.sub(REGEX_PUNCTUATION, '', token)  # also filters %-signs - do we want this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, ['awww', \"that's\", 'a', 'bummer', 'you', 'shoulda', 'got', 'david', 'carr', 'of', 'third', 'day', 'to', 'do', 'it', ';D']), (0, ['is', 'upset', 'that', 'he', \"can't\", 'update', 'his', 'facebook', 'by', 'texting', 'it', 'and', 'might', 'cry', 'as', 'a', 'result', 'school', 'today', 'also', 'blah']), (0, ['i', 'dived', 'many', 'times', 'for', 'the', 'ball', 'managed', 'to', 'save', '50', 'the', 'rest', 'go', 'out', 'of', 'bounds']), (0, ['my', 'whole', 'body', 'feels', 'itchy', 'and', 'like', 'its', 'on', 'fire']), (0, ['no', \"it's\", 'not', 'behaving', 'at', 'all', \"i'm\", 'mad', 'why', 'am', 'i', 'here', 'because', 'i', \"can't\", 'see', 'you', 'all', 'over', 'there']), (0, ['not', 'the', 'whole', 'crew']), (0, ['need', 'a', 'hug']), (0, ['hey', 'long', 'time', 'no', 'see', 'yes', 'rains', 'a', 'bit', 'only', 'a', 'bit', 'LOL', \"i'm\", 'fine', 'thanks', \"how's\", 'you']), (0, ['nope', 'they', \"didn't\", 'have', 'it']), (0, ['que', 'me', 'muera'])]\n"
     ]
    }
   ],
   "source": [
    "tokens_cleaned = [] \n",
    "\n",
    "\n",
    "for tokenlistno, tokenlist in enumerate(tokens):\n",
    "    tokens_cleaned.append([])\n",
    "    #print(\"### Tokenlist: \", tokenlist)\n",
    "    for token in tokenlist:\n",
    "        #print(\"### Current token: \", token)\n",
    "        result = filter_token(token)\n",
    "        if result:\n",
    "            result = remove_punctuation_but_keep_emoticons(result)\n",
    "            if result:  \n",
    "                # keep all-caps words longer than 1 char\n",
    "                if not (result.isupper() and len(result) > 1) and not result.islower():\n",
    "                    result = result.lower()\n",
    "                #print(result)\n",
    "                tokens_cleaned[tokenlistno].append(result)\n",
    "\n",
    "\n",
    "cleaned = list(zip(polarities, tokens_cleaned))\n",
    "print(cleaned[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Zählen Sie die Tokens in Ihrem Datensatz. Benutzen Sie dafür ein Dictionary. Geben Sie die 100 häufigsten Wörter sortiert aus. Was stellen Sie fest? Was müssen Sie zusätzlich noch filtern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " [(\"'thanks'\", 2), ('plaguing', 5), ('wisht', 1), ('yesterdaze', 1), ('crabbin', 1), ('cstw4everlt3', 1), ('nakakamiss', 2), ('shaant', 3), ('triton', 3), ('â05', 1)]\n",
      "\n",
      "\n",
      " [('i', 750477), ('to', 560556), ('the', 515165), ('a', 377409), ('my', 310472), ('and', 294853), ('you', 264649), ('is', 233261), ('it', 227673), ('for', 214265)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tokens_cleaned_flat = [token for tokenlist in tokens_cleaned for token in tokenlist]\n",
    "#print(tokens_cleaned_flat[0:20])\n",
    "word_count = Counter(tokens_cleaned_flat)\n",
    "\n",
    "print(\"\\n\\n\",list(word_count.items())[0:10])\n",
    "\n",
    "print(\"\\n\\n\",word_count.most_common(10)) #word_count_sorted = word_count.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Aufgabe\n",
    "\n",
    "Wie Eingangs erwähnt, beschäftigt sich Sentiment Analysis damit, eine Äußerung automatisch dahingehend zu klassifizieren,\n",
    "ob der Inhalt positiv oder negativ gemeint ist.\n",
    "Im Machine-Learning-Jargon gesprochen hat man es also mit einer binären Klassifikation zu tun. Wir möchten im folgenden Teil ein neuronales Netz trainieren, das entscheiden kann, ob ein Tweet positiv oder negativ gemeint.\n",
    "\n",
    "Für das Training des neuronalen Netzes möchten wir Keras als Framework benutzen. Keras bietet eine Vereinfachung der Tensorflow-API an, d.h. mit deutlich weniger Aufwand kann man alle Funktionalitäten von Tensorflow benutzen.\n",
    "\n",
    "Keras bietet zwei unterschiedliche APIs zum Erstellen von neuronalen Netzen an, namentlich _sequential_ und _functional_.\n",
    "Bei der _sequential_-API wird das Model Schicht für Schicht aufgebaut. Leider kann mit dieser API kein Model aufgebaut werden, das Schichten enthält, die mehr als eine Vorgängerschicht gleichzeitig haben, oder einzelne Schichten wiederbenutzt. Mit der _functional_-API ist dies möglich.\n",
    "\n",
    "Aktuell liegen unsere Daten zwar in tokenisierter und gesäuberter Form vor, allerdings wird ein neuronales Netz damit sehr wenig anfangen können. Wir müssen unsere Daten also noch etwas weiter vorbereiten.\n",
    "\n",
    "Als Eingabe soll unser neuronales Netz später Vektoren nehmen, deren einzelne Komponenten alle Wörter darstellen und jeder Eintrag die Anzahl des Wortes in den jeweiligen Tweets. Ein Beispiel:\n",
    "Zwei Tweets \"lorem ipsum\" und \"foo foo bar\", die Vektoren hätten die Länge 4 und für den ersten Tweet wäre der Vektor `[1,1,0,0]`, für den zweiten `[0,0,2,1]`.\n",
    "\n",
    "1. Befüllen Sie das dictionary `word2idx` so, dass jedes Wort auf einen Index abgebildet wird und die Indizes streng monoton aufsteigend sind. Für das Beispiel oben wäre `word2idx = {\"lorem\": 0, \"ipsum\": 1, \"foo\": 3, \"bar\": 4}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"'thanks'\", 0), ('plaguing', 1), ('wisht', 2), ('yesterdaze', 3), ('crabbin', 4), ('cstw4everlt3', 5), ('nakakamiss', 6), ('shaant', 7), ('triton', 8), ('tiredso', 10)]\n"
     ]
    }
   ],
   "source": [
    "word2idx = {word: idx for idx, word in enumerate(word_count.keys())}\n",
    "\n",
    "print(list(word2idx.items())[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Welche Länge werden die Vektoren haben?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439276\n"
     ]
    }
   ],
   "source": [
    "VECTOR_LEN = len(word2idx)\n",
    "print(VECTOR_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Wir könnten mit `numpy` ein Array befüllen, das für jeden der 16 Millionen Tweets einen Vektor wie oben beschrieben enthält. \n",
    "Bevor Sie damit beginnen, überschlagen Sie, wieviel Speicherplatz (im Hauptspeicher) ein solches Array belegen würde, wenn jeder Eintrag 32 bit hat. Reicht Ihr Hauptspeicher dafür aus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28113.664 GB\n"
     ]
    }
   ],
   "source": [
    "MEMORY = 32 * VECTOR_LEN * 16000000\n",
    "print(MEMORY/8/1000/1000/1000,\"GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Um das Problem mit dem zu kleinen Hauptspeicher zu umgehen, bietet Keras die Möglichkeit, anstatt auf einem kompletten Datensatz zu operieren, immer nur kleinere Häppchen abzuarbeiten. Dazu wird ein Python-Generator eingesetzt.\n",
    "Vervollständigen Sie die Funktion unten, so dass ein Generator entsteht. Die Parameter der Funktion sind:\n",
    " * d: tokenisierte und gesäuberte Tweets und Labels\n",
    " * w2i: das word2index dictionary\n",
    " * batch_size: Anzahl der vektorisierten Tweets, die pro Aufruf zurückgegeben werden sollen.\n",
    " \n",
    "Die benutzen Tweets nacheinander aus `d` gewählt werden und kein Tweet mehrfach zurückgegeben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagofwords(tokens, w2i):\n",
    "    bow = np.zeros((len(w2i)), dtype=np.int)\n",
    "      \n",
    "    for token in tokens:\n",
    "        idx = w2i[token]\n",
    "        bow[idx] += 1\n",
    "    \n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def data_generator(d, w2i, batch_size):\n",
    "    l = len(d)\n",
    "    for start_index in range(0, l, batch_size):\n",
    "        batch = d[start_index:min(start_index+batch_size, l)]\n",
    "        \n",
    "        batch_x = np.zeros((batch_size, len(w2i.keys())), dtype=np.int)\n",
    "        batch_y = np.zeros((batch_size, 1), dtype=np.int)\n",
    "        # TODO\n",
    "        for rowno, row in enumerate(batch):\n",
    "            batch_x[rowno] = bagofwords(row[1], w2i)\n",
    "            batch_y[rowno] = row[0]\n",
    "          \n",
    "        yield batch_x, batch_y\n",
    "    yield batch_x, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie können Ihren Generator wie folgt ausprobieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_data_generator(d, w2i, batch_size):\n",
    "    l = len(d)\n",
    "    for start_index in range(0, l, batch_size):\n",
    "        batch = d[start_index:min(start_index+batch_size, l)]\n",
    "          \n",
    "        yield batch\n",
    "    yield batch\n",
    "        \n",
    "print(len(cleaned))\n",
    "gen2 = debug_data_generator(cleaned, word2idx, 1600)\n",
    "for i in range(0,1500):\n",
    "    t = next(gen2)\n",
    "    if i>990:\n",
    "        print(i, 1600*i)\n",
    "        print(len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = data_generator(cleaned, word2idx, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]]), array([[0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0]]))\n"
     ]
    }
   ],
   "source": [
    "print(next(gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir sind nun endlich soweit, unser neuronales Netz aufzubauen. Da unser Netz genau ein hidden Layer hat und auch sonst nicht sonderlich komplex ist, benutzen wir die _Sequential_-API von Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "m = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Fügen Sie einen _Dense_-Layer dem Netz hinzu, als _hidden units_ können Sie 16 nehmen. Da dies auch der Eingabe-Layer ist, müssen Sie den Parameter `input_shape` definieren. (Siehe auch: https://keras.io/layers/core/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "m.add(Dense(16, input_shape=(VECTOR_LEN,), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Als letzten Layer in unserem neuronalen Netz, fügen Sie einen weiteren _Dense_-Layer hinzu. Dieser Layer dient auch als \"Ausgabelayer\" Überlegen Sie sich die Anzahl der _hidden units_ (Hinweis: Wie lässt sich unser Machine-Learning-Problem kategorisieren?) Welche _Activation_-Funktion wählen Sie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Kompilieren Sie das neuronale Netz. Als `optimizer` können Sie 'adam' benutzen. Wählen Sie eine passende `loss`-Funktion aus. Begründen Sie Ihre Entscheidung. (https://keras.io/models/model/#compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#https://machinelearningmastery.com/deep-learning-bag-of-words-model-sentiment-analysis/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Bevor Sie nun das neuronale Netz trainieren, teilen Sie noch Ihren Datensatz in zwei Teile auf. Einen Teil zum Trainieren und einen zum Evaluieren. Das Verhältnis der beiden Datensätze sollte 70%:30% sein. Bevor Sie die Daten aufteilen, durchmischen Sie sie mit der `shuffle`-Methode aus dem `random`-Modul. Außerdem sollten Sie die Datenmenge zunächst auf ca. 100000 begrenzen, damit das Training des neuronalen Netzes nicht ewig dauert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "cleaned_shuffled = cleaned[:]\n",
    "shuffle(cleaned_shuffled)\n",
    "\n",
    "cleaned_train = cleaned_shuffled[:int(len(cleaned_shuffled)*0.7)]\n",
    "cleaned_eval = cleaned_shuffled[int(len(cleaned_shuffled)*0.7):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Wir sind nun soweit das neuronale Netz zu trainieren. Da wir den oben entwickelten Generator einsetzen wollen, verwenden wir dazu die `fit_generator`-Methode. Als `batch_size` können Sie 100 nehmen, für den `epochs`-Parameter 10. Was wählen Sie als `steps_per_epoch`-Parameter? (https://keras.io/models/model/#fit_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "11200/11200 [==============================] - 3537s 316ms/step - loss: 0.4459 - acc: 0.7947\n",
      "Epoch 2/10\n",
      "    1/11200 [..............................] - ETA: 44:27 - loss: 0.3529 - acc: 0.8200WARNING:tensorflow:Your dataset iterator ran out of data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdf65221400>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit_generator(data_generator(cleaned_train, word2idx, 100), epochs=10, steps_per_epoch=int(np.floor(len(cleaned_train) / 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Während das Netz trainiert wird, können Sie sich Gedanken zur Evaluierung machen:\n",
    "   * Definieren Sie die üblichen Fehlerklassen (wahr positiv, falsch positiv, wahr negativ, falsch negativ)\n",
    "   * Eine häufig benutzte Evaluationsmetrik ist die _Accuracy_. Beschreiben Sie dieses Metrik und schreiben Sie die Formel zur Berechnung auf.\n",
    "   * Warum könnte die _Accuracy_ eine schlechte Metrik sein?\n",
    "   * Zur Evaluation von binären Klassifikationsproblemen wird in der Literatur gerne _Precision_ und _Recall_ verwendet. Wie sind die beiden Evaluationsmaße definiert? Beschreiben Sie diese Metriken mit eigenen Worten. Schreiben Sie auch die Formeln zur Berechnung auf.\n",
    "   * Warum könnten _Precision_ und _Recall_ bessere Metriken sein als _Accuracy_?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Inzwischen sollte das Netz fertig trainiert sein. Speichern Sie es ab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save('my_net_1_epochs_7947.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load saved model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.utils import CustomObjectScope\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "with CustomObjectScope({'GlorotUniform': glorot_uniform()}):\n",
    "    model = load_model('my_net_1_epochs_7947.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Evaluieren Sie ihr Netz mit dem Datensatz, den Sie oben beseite gelegt haben. Benutzen Sie dafür die `predict_classes`-Methode des Models. Berechnen Sie dafür _Precision_, _Recall_ und _Accuracy_. Interpretieren Sie kurz Ihre Ergebnisse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 439276\n"
     ]
    }
   ],
   "source": [
    "print(len(cleaned_eval[0:100]), len(word2idx.keys()))\n",
    "\n",
    "# yields MemoryError\n",
    "\n",
    "#cleaned_eval_x = np.zeros((len(cleaned_eval), len(word2idx.keys())), dtype=np.int)\n",
    "#cleaned_eval_y = np.zeros((len(cleaned_eval), 1), dtype=np.int)\n",
    "        # TODO\n",
    "#for rowno, row in enumerate(cleaned_eval):\n",
    "#    cleaned_eval_x[rowno] = bagofwords(row[1], word2idx)\n",
    "#    cleaned_eval_y[rowno] = row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned_eval_predictions = model.predict_classes(eval_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run predictions on the evaluation dataset to calculate precision, recall and accuracy later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-3:\n",
      "Traceback (most recent call last):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-dc8f6b457be8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_eval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   1520\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m             verbose=verbose)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mpredict_generator\u001b[0;34m(model, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;31m# Compatibility with the generators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/keras/utils/data_utils.py\", line 626, in next_sample\n",
      "    return six.next(_SHARED_SEQUENCES[uid])\n",
      "  File \"<ipython-input-12-b57c3a6bf6bd>\", line 13, in data_generator\n",
      "    batch_x[rowno] = bagofwords(row[1], w2i)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_generator(data_generator(cleaned_eval, word2idx, 100), steps=int(np.ceil(len(cleaned_eval) / 100))-1, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save predictions to file for repeated use without having to run the prediction over and over again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('predictions_1_7947', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_one_hundred_predictions = model.predict_generator(data_generator(cleaned_eval[-100:], word2idx, 100), steps=1)\n",
    "print(len(last_one_hundred_predictions))\n",
    "print(type(last_one_hundred_predictions))\n",
    "print(last_one_hundred_predictions[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = np.load('predictions_1_7947.npy')\n",
    "print(loaded[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze this one and don't run it anymore (only used to check similarity when first saving the predictions)\n",
    "print(loaded[0:10])\n",
    "print(np.array_equal(y_pred, loaded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(loaded)\n",
    "np.concatenate((loaded, last_one_hundred_predictions), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_full = np.concatenate((loaded, last_one_hundred_predictions), axis=0)\n",
    "rounded_predictions = np.rint(loaded_full)\n",
    "rounded_predictions = rounded_predictions.flatten().astype(int)\n",
    "print(rounded_predictions[0:10])\n",
    "original_classes_eval =[i[0] for i in cleaned_eval]\n",
    "original_classes_eval = np.asarray(original_classes_eval)\n",
    "print(original_classes_eval[0:10])\n",
    "print(rounded_predictions.shape)\n",
    "print(original_classes_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Positive (TP): we predict a label of 1 (positive), and the true label is 1.\n",
    "true_positives = np.sum(np.logical_and(rounded_predictions == 1, original_classes_eval == 1))\n",
    "\n",
    "# True Negatives (TN): we predict a label of 0 (negative), and the true label is 0.\n",
    "true_negatives = np.sum(np.logical_and(rounded_predictions == 0, original_classes_eval == 0))\n",
    " \n",
    "# False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n",
    "false_positives = np.sum(np.logical_and(rounded_predictions == 1, original_classes_eval == 0))\n",
    " \n",
    "# False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n",
    "false_negatives = np.sum(np.logical_and(rounded_predictions == 0, original_classes_eval == 1))\n",
    "\n",
    "print('TP: %i, FP: %i, TN: %i, FN: %i' % (true_positives,false_positives, true_negatives, false_negatives))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Precision = \\frac{True~positives}{True~positives + False~positives} $$ \n",
    "\n",
    "$$Recall = \\frac{True~positives}{True~positives + False~negatives} = \\frac{True~positives}{Total~actual~positives} $$ \n",
    "\n",
    "$$F1 Score = 2 x \\frac{Precision * Recall}{Precision + Recall} $$\n",
    "\n",
    "$$Accuracy = \\frac{True~positives + True~negatives}{True~positives + True~negatives + False~positives + False~negatives} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = (true_positives) / (true_positives + false_positives)\n",
    "recall = (true_positives) / (true_positives + false_negatives)\n",
    "f1_score = 2 * ((precision * recall) / (precision + recall))\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
    "\n",
    "\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hausaufgabe\n",
    "1. Trainieren Sie Ihr Netz auf dem großen Datensatz.\n",
    "2. Verändern Sie die Parameter Ihres Netzes (z.B Anzahl _hidden units_, Anzahl _hidden layers_) und trainieren Sie das Netz erneut (auf dem kleinen Datensatz). Was stellen Sie fest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
